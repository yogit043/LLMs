{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BEGINNER PRE-REQUISITES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Natural Language Processing (NLP)** is a subfield of Artificial Intelligence (AI) and computer science that focuses on the interaction between computers and human (natural) languages. The goal of NLP is to enable machines to understand, interpret, generate, and respond to human language in a way that is both meaningful and useful.\n",
    "\n",
    "In essence, NLP bridges the gap between human communication and computer understanding, allowing machines to read, hear, and make sense of the vast amounts of human language data. This field combines computational linguistics, machine learning, and deep learning to process, analyze, and generate natural language text or speech.\n",
    "\n",
    "![nlpintro](res/nlp_intro.png)\n",
    "\n",
    "### Key Components of NLP\n",
    "\n",
    "1. **Syntax**: Syntax refers to the rules that govern the structure of sentences. In NLP, syntactic analysis (or parsing) is used to assess whether a sentence conforms to these grammatical rules. It helps the machine identify the roles of words (such as subjects, verbs, and objects) within a sentence.\n",
    "\n",
    "2. **Semantics**: Semantics involves understanding the meaning of words and sentences. Semantic analysis allows the machine to grasp context and disambiguate words with multiple meanings. For example, understanding that the word \"bank\" could refer to a financial institution or the side of a river, depending on the context.\n",
    "\n",
    "3. **Morphology**: This is the study of the structure and form of words, such as roots, prefixes, and suffixes. Morphological analysis breaks words down into their constituent parts to derive meaning.\n",
    "\n",
    "4. **Pragmatics**: Pragmatics deals with how language is used in practice and how context influences meaning. It helps machines understand indirect language, idioms, and implications behind statements.\n",
    "\n",
    "5. **Phonology and Speech**: For spoken language processing, NLP also incorporates phonology, which involves the sounds of speech and how they relate to language.\n",
    "\n",
    "### Common Tasks in NLP\n",
    "\n",
    "NLP tasks range from simple text processing to complex understanding and generation of language. Some common tasks include:\n",
    "\n",
    "![applications](res/nlp_applications.jpg)\n",
    "\n",
    "1. **Text Classification**: Assigning predefined categories to text data, such as spam detection in emails or sentiment analysis of product reviews.\n",
    "  \n",
    "2. **Named Entity Recognition (NER)**: Identifying and classifying named entities in text, such as names of people, organizations, dates, and locations.\n",
    "  \n",
    "3. **Machine Translation**: Automatically translating text from one language to another. Popular applications include Google Translate and multilingual support in AI systems.\n",
    "\n",
    "4. **Part-of-Speech (POS) Tagging**: Labeling words in a sentence as nouns, verbs, adjectives, etc., to understand their grammatical role.\n",
    "\n",
    "5. **Sentiment Analysis**: Determining the sentiment or emotion conveyed in a piece of text, often used in analyzing social media posts or product reviews.\n",
    "\n",
    "6. **Question Answering**: Building systems that can understand a query and provide the correct answer based on a body of knowledge. This is the foundation of systems like IBM Watson or OpenAI's GPT models.\n",
    "\n",
    "7. **Speech Recognition and Generation**: Converting spoken language into text (speech-to-text) and generating spoken language from text (text-to-speech).\n",
    "\n",
    "8. **Text Summarization**: Automatically generating a shorter version of a long text while retaining the essential meaning.\n",
    "\n",
    "9. **Coreference Resolution**: Identifying when two or more expressions in a text refer to the same entity (e.g., “John” and “he” in a passage).\n",
    "\n",
    "10. **Language Modeling**: Predicting the next word in a sentence, essential for tasks like autocomplete and text generation.\n",
    "\n",
    "### Techniques in NLP\n",
    "\n",
    "The field of NLP has evolved significantly, driven by advancements in machine learning and deep learning. Some of the most prominent techniques and models used in NLP today include:\n",
    "\n",
    "1. **Rule-based Methods**: Early NLP systems relied on handcrafted rules to process language. These were limited in scalability and adaptability.\n",
    "\n",
    "2. **Machine Learning**: Modern NLP uses supervised and unsupervised learning techniques to model language. Algorithms like Naive Bayes, Support Vector Machines (SVM), and decision trees are applied to NLP tasks like classification and clustering.\n",
    "\n",
    "3. **Deep Learning**: Neural networks, especially **Recurrent Neural Networks (RNNs)** and **Convolutional Neural Networks (CNNs)**, are widely used in NLP to handle tasks that involve sequential data like text. Variants like Long Short-Term Memory (LSTM) networks have been effective in capturing long-range dependencies in text.\n",
    "\n",
    "4. **Transformers**: The introduction of the **Transformer architecture** revolutionized NLP. Transformers, unlike RNNs, can process words in parallel and better capture the context. Models like **BERT (Bidirectional Encoder Representations from Transformers)**, **GPT (Generative Pre-trained Transformer)**, and **T5 (Text-to-Text Transfer Transformer)** are based on this architecture, achieving state-of-the-art results in many NLP tasks.\n",
    "\n",
    "5. **Pre-trained Language Models**: The development of large-scale pre-trained language models has transformed the NLP landscape. These models are pre-trained on vast amounts of text data and then fine-tuned on specific tasks. Some notable examples include BERT, GPT-3, and MBart. These models have made NLP accessible for applications like chatbots, translation, and even creative writing.\n",
    "\n",
    "### Challenges in NLP\n",
    "\n",
    "Despite the advancements, there are several challenges that remain in NLP:\n",
    "\n",
    "1. **Ambiguity**: Language is inherently ambiguous. A single word can have multiple meanings, and a sentence's meaning can change based on context, tone, or cultural factors.\n",
    "\n",
    "2. **Contextual Understanding**: While recent models like Transformers have improved context capture, maintaining long-term context in lengthy conversations or documents is still a challenge.\n",
    "\n",
    "3. **Multilinguality**: Developing NLP systems that work seamlessly across multiple languages is complex. Languages differ in grammar, syntax, and cultural references, making universal models difficult to create.\n",
    "\n",
    "4. **Bias in Language Models**: Since many NLP models are trained on large datasets scraped from the internet, they can inadvertently learn and perpetuate biases present in the data. Efforts are being made to make these models more equitable.\n",
    "\n",
    "5. **Common-Sense Reasoning**: While models can excel at understanding patterns in language, they often struggle with common-sense reasoning. They lack real-world knowledge that humans take for granted.\n",
    "\n",
    "### Applications of NLP\n",
    "\n",
    "NLP is applied across various industries and technologies:\n",
    "\n",
    "- **Search Engines**: NLP is at the core of search algorithms, helping understand and rank relevant results for user queries.\n",
    "  \n",
    "- **Virtual Assistants**: Systems like Amazon Alexa, Google Assistant, and Siri use NLP to understand spoken commands and respond appropriately.\n",
    "\n",
    "- **Customer Service**: Chatbots and virtual assistants in customer support use NLP to handle inquiries and provide solutions automatically.\n",
    "\n",
    "- **Healthcare**: NLP helps in medical data processing, summarizing patient records, and assisting with clinical decision support systems.\n",
    "\n",
    "- **Social Media Analysis**: Sentiment analysis and trend detection in social media platforms rely on NLP to interpret user interactions.\n",
    "\n",
    "- **Legal and Finance**: NLP is used to process legal documents, financial reports, and automate contract analysis.\n",
    "\n",
    "### Summary\n",
    "\n",
    "Natural Language Processing is a dynamic and rapidly evolving field that aims to give machines the ability to understand, interpret, and generate human language. From rule-based systems to modern deep learning models like Transformers, NLP has come a long way and is now central to many technologies we interact with daily. However, there are still challenges related to ambiguity, multilinguality, and bias that researchers continue to address as the field advances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. NLP Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NLP pipeline consists of several stages that transform raw text into structured data suitable for analysis or model input. Here's a detailed breakdown:\n",
    "\n",
    "![image](res/nlp-pipeline.webp)\n",
    "\n",
    "1. **Data Collection**:\n",
    "\n",
    "   **What:** Gathering raw text data from various sources.\n",
    "\n",
    "   **When:** At first to collect the data for training\n",
    "\n",
    "   **How:** Web scraping, APIs, databases, user-generated content\n",
    "\n",
    "   **Why:** To obtain the necessary input for analysis and model training.\n",
    "\n",
    "2. **Data Processing (Cleaning, Tokenization, etc.)**:\n",
    "\n",
    "   **What:** Preparing the raw text data for analysis.\n",
    "\n",
    "   **When:** Before building the model to ensure that data is good for model to be trained on\n",
    "\n",
    "   **How:** Through various text processing methods (detailed in the next section)\n",
    "\n",
    "   **Why:** To improve data quality and consistency for better results.\n",
    "\n",
    "3. **Model Building**:\n",
    "\n",
    "   **What:** Creating and training NLP models on the processed data.\n",
    "\n",
    "   **When:** Once data is created model should be created along with the training paradigm\n",
    "\n",
    "   **How:** Using machine learning algorithms, deep learning architectures, or pre-trained models\n",
    "\n",
    "   **Why:** To develop models capable of understanding and generating natural language.\n",
    "\n",
    "4. **Inference/Testing**:\n",
    "\n",
    "   **What:** Using the trained model to make predictions or generate output.\n",
    "\n",
    "   **When:** Once model is trained, to use the model for getting results on new data\n",
    "\n",
    "   **How:** Feeding new data into the trained model and interpreting the output\n",
    "\n",
    "   **Why:** To apply the model to new, unseen data and obtain results.\n",
    "\n",
    "5. **Deployment**:\n",
    "\n",
    "   **What:** Making the NLP model available for use in real-world applications.\n",
    "\n",
    "   **When:** After rigorous testing and inferencing deployed for real world users to use\n",
    "\n",
    "   **How:** Through APIs, cloud services, or on-premises infrastructure\n",
    "\n",
    "   **Why:** To integrate the NLP capabilities into products or services.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Text processing is a crucial step in the NLP pipeline, occurring after data collection and before model building. It's essential for cleaning and preparing raw text data, improving data quality and consistency for better analysis and model performance. \n",
    "\n",
    "![methods](res/text_preprocessing.png)\n",
    "\n",
    "\n",
    "Here's a detailed breakdown of text processing methods, including regex, in the order they're typically applied:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Processing Methods: What, When, Why, How"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1. Text Normalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](res/text_normalization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "   **What:** Converting text to a standard format (e.g., lowercase).\n",
    "    \n",
    "   **When:** At the beginning of text processing.\n",
    "    \n",
    "   **Why:** To reduce variability and ensure consistency in the text.\n",
    "    \n",
    "   **How:** Using string methods or custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THIS IS A SAMPLE TEXT.\n"
     ]
    }
   ],
   "source": [
    "text = \"ThIs Is A SaMpLe TeXt.\"\n",
    "normalized_text = text.upper()\n",
    "print(normalized_text)  # Output: \"this is a sample text.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2. Regular Expressions (Regex)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](res/regex_intro.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   **What:** Pattern-based text cleaning and extraction.\n",
    "    \n",
    "   **When:** Early in the text processing pipeline, often right after or combined with normalization.\n",
    "    \n",
    "   **Why:** To remove or modify specific patterns in text (e.g., URLs, special characters).\n",
    "    \n",
    "   **How:** Using the `re` module in Python to define and apply patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check out this link: \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"Check out this link: https://www.example.com\"\n",
    "clean_text = re.sub(r'http\\S+', '', text)\n",
    "print(clean_text)  # Output: \"Check out this link: \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  **3. Tokenization**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](res/tokenization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "   **What:** Breaking text into smaller units (tokens), usually words or subwords.\n",
    "    \n",
    "   **When:** After initial cleaning with regex and normalization.\n",
    "    \n",
    "   **Why:** To prepare text for further analysis and to work with individual units of meaning.\n",
    "    \n",
    "   **How:** Using specialized libraries like NLTK or spaCy, or custom functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLP', 'is', 'fascinating', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text = \"NLP is fascinating!\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)  # Output: ['NLP', 'is', 'fascinating', '!']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4. Stop Words Removal**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![res](res/stopwords1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "   **What:** Eliminating common words that don't carry significant meaning.\n",
    "    \n",
    "   **When:** After tokenization, when focusing on content words is important.\n",
    "    \n",
    "   **Why:** To reduce noise in the text and focus on meaningful words.\n",
    "    \n",
    "   **How:** Using predefined lists of stop words or custom lists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some common stop words are provided here:\n",
    "\n",
    "![image](res/stopwords2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample', 'sentence', 'stop', 'words', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "text = \"This is a sample sentence with stop words.\"\n",
    "tokens = word_tokenize(text)\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "print(filtered_tokens)  # Output: ['This', 'sample', 'sentence', 'stop', 'words', '.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **5. Stemming**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "   **What:** Reducing words to their root form by removing suffixes.\n",
    "    \n",
    "   **When:** After stop words removal, when quick normalization is needed without preserving exact meaning.\n",
    "    \n",
    "   **Why:** To simplify text and reduce vocabulary size.\n",
    "    \n",
    "   **How:** Using rule-based algorithms like Porter or Snowball stemmers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'run', 'ran']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "words = [\"running\", \"runs\", \"ran\"]\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "print(stemmed_words)  # Output: ['run', 'run', 'ran']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **6. Lemmatization**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "   **What:** Reducing words to their base or dictionary form (lemma).\n",
    "    \n",
    "   **When:** After stop words removal, when you need to normalize words while preserving meaning.\n",
    "    \n",
    "   **Why:** To reduce word variations and improve text analysis accuracy.\n",
    "    \n",
    "   **How:** Using morphological analysis and dictionaries, often with part-of-speech information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a comparison of stemming vs lemmatization\n",
    "\n",
    "![image](res/stemminglemmatization.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['betterness', 'running', 'ate']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = [\"betterness\", \"running\", \"ate\"]\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "print(lemmatized_words)  # Output: ['better', 'running', 'eat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Detailed Explanation of Regex in Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular expressions (regex) play a crucial role in text processing, often being used early in the pipeline to clean and standardize text. Here's a more detailed look at regex in text processing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **What:**\n",
    "Regex is a sequence of characters that define a search pattern. It's a powerful tool for pattern matching and text manipulation.\n",
    "\n",
    "#### **When:**\n",
    "- During initial text cleaning, often right after or combined with normalization.\n",
    "- Throughout the text processing pipeline for specific cleaning tasks.\n",
    "\n",
    "#### **Why:**\n",
    "- To efficiently remove or modify specific patterns in text.\n",
    "- To extract particular pieces of information from text.\n",
    "- To standardize text format and structure.\n",
    "\n",
    "#### **How:**\n",
    "Using the `re` module in Python to define patterns and apply them to text. Common methods include:\n",
    "\n",
    "1. **re.sub()**: For substituting patterns with replacement text.\n",
    "2. **re.findall()**: For finding all occurrences of a pattern.\n",
    "3. **re.search()**: For finding the first occurrence of a pattern.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Common Regex Operations in Text Processing:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **Removing Special Characters**:\n",
    "   ```python\n",
    "   import re\n",
    "   text = \"Hello! How are you? I'm doing great. #happy\"\n",
    "   clean_text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "   print(clean_text)  # Output: \"Hello How are you Im doing great happy\"\n",
    "   ```\n",
    "\n",
    "2. **Removing Extra Whitespace**:\n",
    "   ```python\n",
    "   import re\n",
    "   text = \"This   has   extra   spaces.\"\n",
    "   clean_text = re.sub(r'\\s+', ' ', text).strip()\n",
    "   print(clean_text)  # Output: \"This has extra spaces.\"\n",
    "   ```\n",
    "\n",
    "3. **Extracting Emails**:\n",
    "   ```python\n",
    "   import re\n",
    "   text = \"Contact us at info@example.com or support@example.com\"\n",
    "   emails = re.findall(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', text)\n",
    "   print(emails)  # Output: ['info@example.com', 'support@example.com']\n",
    "   ```\n",
    "\n",
    "4. **Removing HTML Tags**:\n",
    "   ```python\n",
    "   import re\n",
    "   html = \"<p>This is <b>bold</b> text</p>\"\n",
    "   clean_text = re.sub(r'<[^>]+>', '', html)\n",
    "   print(clean_text)  # Output: \"This is bold text\"\n",
    "   ```\n",
    "\n",
    "5. **Standardizing Phone Numbers**:\n",
    "   ```python\n",
    "   import re\n",
    "   phone = \"Call us at 1234567890 or (123) 456-7890\"\n",
    "   standardized = re.sub(r'\\D', '', phone)\n",
    "   print(standardized)  # Output: \"12345678901234567890\"\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Integration with Other Text Processing Steps:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regex is often used in combination with other text processing methods. For example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. **Regex + Tokenization**:\n",
    "   ```python\n",
    "   import re\n",
    "   from nltk.tokenize import word_tokenize\n",
    "   \n",
    "   text = \"Hello! How are you? I'm doing great. #happy\"\n",
    "   clean_text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "   tokens = word_tokenize(clean_text)\n",
    "   print(tokens)  # Output: ['Hello', 'How', 'are', 'you', 'Im', 'doing', 'great', 'happy']\n",
    "   ```\n",
    "\n",
    "2. **Regex + Stop Words Removal**:\n",
    "   ```python\n",
    "   import re\n",
    "   from nltk.corpus import stopwords\n",
    "   \n",
    "   stop_words = set(stopwords.words('english'))\n",
    "   text = \"Hello! How are you? I'm doing great. #happy\"\n",
    "   clean_text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "   words = clean_text.split()\n",
    "   filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "   print(filtered_words)  # Output: ['Hello', 'great', 'happy']\n",
    "   ```\n",
    "\n",
    "By integrating regex into the text processing pipeline, you can effectively clean and standardize text before applying more advanced NLP techniques. This helps improve the quality of your data and the performance of subsequent analysis or modeling steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings are dense vector representations of words or phrases that capture semantic meaning. Here are the main embedding methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Significance of Embeddings**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Capture semantic relationships between words\n",
    "- Reduce dimensionality of text data\n",
    "- Improve performance of many NLP tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Embedding methods**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1. Bag of Words (BoW)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What:** Representing text as a vector of word counts.\n",
    "\n",
    "**When:** For simple text classification or clustering tasks.\n",
    "\n",
    "**Why:** Easy to implement and understand.\n",
    "\n",
    "**How:** Creating a vocabulary and counting word occurrences in each document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](res/bow.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "corpus = [\"This is the first document.\", \"This document is the second document.\"]\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2. TF-IDF (Term Frequency-Inverse Document Frequency)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What:** Representing the importance of words in a document relative to a collection.\n",
    "\n",
    "**When:** When you need to consider the relevance of words across documents.\n",
    "\n",
    "**Why:** Provides a better representation of word importance than simple counts.\n",
    "\n",
    "**How:** Calculating term frequency and inverse document frequency for each word and applies the following mathematical function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](res/tfidf.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.4090901  0.57496187 0.4090901  0.         0.4090901  0.4090901 ]\n",
      " [0.66758217 0.         0.33379109 0.46913173 0.33379109 0.33379109]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "corpus = [\"This is the first document.\", \"This document is the second document.\"]\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3. Word2Vec**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What:** Creating dense vector representations of words using neural networks \n",
    " \n",
    "**When:** When you need to capture semantic relationships between words.\n",
    " \n",
    "**Why:** Produces word embeddings that capture context and semantic meaning.\n",
    " \n",
    "**How:** Trained a shallow neural network on large text corpora based on words near it using methods like CBOW."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](res/word2vec.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9.4563962e-05  3.0773198e-03 -6.8126451e-03 -1.3754654e-03\n",
      "  7.6685809e-03  7.3464094e-03 -3.6732971e-03  2.6427018e-03\n",
      " -8.3171297e-03  6.2054861e-03 -4.6373224e-03 -3.1641065e-03\n",
      "  9.3113566e-03  8.7338570e-04  7.4907029e-03 -6.0740625e-03\n",
      "  5.1605068e-03  9.9228229e-03 -8.4573915e-03 -5.1356913e-03\n",
      " -7.0648370e-03 -4.8626517e-03 -3.7785638e-03 -8.5361991e-03\n",
      "  7.9556061e-03 -4.8439382e-03  8.4236134e-03  5.2625705e-03\n",
      " -6.5500261e-03  3.9578713e-03  5.4701497e-03 -7.4265362e-03\n",
      " -7.4057197e-03 -2.4752307e-03 -8.6257253e-03 -1.5815723e-03\n",
      " -4.0343284e-04  3.2996845e-03  1.4418805e-03 -8.8142155e-04\n",
      " -5.5940580e-03  1.7303658e-03 -8.9737179e-04  6.7936908e-03\n",
      "  3.9735902e-03  4.5294715e-03  1.4343059e-03 -2.6998555e-03\n",
      " -4.3668128e-03 -1.0320747e-03  1.4370275e-03 -2.6460087e-03\n",
      " -7.0737829e-03 -7.8053069e-03 -9.1217868e-03 -5.9351693e-03\n",
      " -1.8474245e-03 -4.3238713e-03 -6.4606704e-03 -3.7173224e-03\n",
      "  4.2891586e-03 -3.7390434e-03  8.3781751e-03  1.5339935e-03\n",
      " -7.2423196e-03  9.4337985e-03  7.6312125e-03  5.4932819e-03\n",
      " -6.8488456e-03  5.8226790e-03  4.0090932e-03  5.1853694e-03\n",
      "  4.2559016e-03  1.9397545e-03 -3.1701624e-03  8.3538452e-03\n",
      "  9.6121803e-03  3.7926030e-03 -2.8369951e-03  7.1275235e-06\n",
      "  1.2188185e-03 -8.4583247e-03 -8.2239453e-03 -2.3101569e-04\n",
      "  1.2372875e-03 -5.7433806e-03 -4.7252737e-03 -7.3460746e-03\n",
      "  8.3286157e-03  1.2129784e-04 -4.5093987e-03  5.7017053e-03\n",
      "  9.1800150e-03 -4.0998720e-03  7.9646818e-03  5.3754342e-03\n",
      "  5.8791232e-03  5.1259040e-04  8.2130842e-03 -7.0190406e-03]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "vector = model.wv[\"dog\"]\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4. Custom Embedding Creation with ANN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What:** Creating task-specific word embeddings using artificial neural networks.\n",
    " \n",
    "**When:** When pre-trained embeddings don't capture the specific nuances of your domain.\n",
    " \n",
    "**Why:** To create embeddings tailored to your specific task or domain.\n",
    " \n",
    "**How:** Training a neural network with an embedding layer on your specific dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](res/annembedding.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 50])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CustomEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "\n",
    "vocab_size = 1000\n",
    "embedding_dim = 50\n",
    "model = CustomEmbedding(vocab_size, embedding_dim)\n",
    "input_ids = torch.tensor([1, 2, 3, 4, 5])\n",
    "embeddings = model(input_ids)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. NLP with Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This section covers the integration of NLP techniques with machine learning models.\n",
    "\n",
    "1. **Introduction to Machine Learning**:\n",
    "   - What: A subset of AI that focuses on creating systems that can learn from and make decisions based on data.\n",
    "   - Why: To develop models that can automatically improve their performance through experience.\n",
    "\n",
    "2. **Types of ML models**:\n",
    "\n",
    "   a. **Supervised Learning**:\n",
    "      - What: Learning from labeled data to predict outcomes for new, unseen data.\n",
    "      - Examples: Classification (e.g., spam detection), Regression (e.g., price prediction)\n",
    "\n",
    "   b. **Unsupervised Learning**:\n",
    "      - What: Finding patterns or structures in unlabeled data.\n",
    "      - Examples: Clustering (e.g., customer segmentation), Dimensionality Reduction (e.g., PCA)\n",
    "\n",
    "   c. **Semi-supervised Learning**:\n",
    "      - What: Learning from a combination of labeled and unlabeled data.\n",
    "      - When: When you have a small amount of labeled data and a large amount of unlabeled data.\n",
    "\n",
    "   d. **Reinforcement Learning**:\n",
    "      - What: Learning through interaction with an environment to maximize a reward signal.\n",
    "      - Examples: Game playing (e.g., AlphaGo), Robotics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](res/ml.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### **Introduction**\n",
    "\n",
    "Naive Bayes is a popular algorithm for text classification. It's based on Bayes' Theorem and makes a \"naive\" assumption that all features (words, in our case) are independent of each other.\n",
    "\n",
    "#### **Bayes' Theorem**\n",
    "\n",
    "Bayes' Theorem is expressed as:\n",
    "\n",
    "```math\n",
    "P(A|B) = (P(B|A) * P(A)) / P(B)\n",
    "```\n",
    "\n",
    "Where:\n",
    "- P(A|B) is the probability of A given B has occurred\n",
    "- P(B|A) is the probability of B given A has occurred\n",
    "- P(A) is the probability of A occurring\n",
    "- P(B) is the probability of B occurring\n",
    "\n",
    "#### **Naive Bayes for Text Classification**\n",
    "\n",
    "Let's break down how Naive Bayes works for text classification with a simple example.\n",
    "\n",
    "##### **Step 1: Data Preparation**\n",
    "\n",
    "Imagine we have a small dataset for sentiment analysis:\n",
    "\n",
    "| Text                | Sentiment |\n",
    "|---------------------|-----------|\n",
    "| \"great product\"     | Positive  |\n",
    "| \"bad quality\"       | Negative  |\n",
    "| \"excellent service\" | Positive  |\n",
    "| \"terrible experience\"| Negative |\n",
    "\n",
    "##### **Step 2: Vocabulary Building**\n",
    "\n",
    "We create a vocabulary of unique words:\n",
    "{great, product, bad, quality, excellent, service, terrible, experience}\n",
    "\n",
    "##### **Step 3: Calculate Prior Probabilities**\n",
    "\n",
    "P(Positive) = 2/4 = 0.5\n",
    "P(Negative) = 2/4 = 0.5\n",
    "\n",
    "##### **Step 4: Calculate Likelihood Probabilities**\n",
    "\n",
    "For each word in each class:\n",
    "\n",
    "```math\n",
    "P(word|class) = (count of word in class + 1) / (total words in class + vocabulary size)\n",
    "```\n",
    "\n",
    "We add 1 to the numerator and vocabulary size to the denominator for smoothing.\n",
    "\n",
    "**For Positive class:**\n",
    "- P(great|Positive) = (1 + 1) / (4 + 8) = 2/12 = 0.167\n",
    "- P(product|Positive) = (1 + 1) / (4 + 8) = 2/12 = 0.167\n",
    "- P(excellent|Positive) = (1 + 1) / (4 + 8) = 2/12 = 0.167\n",
    "- P(service|Positive) = (1 + 1) / (4 + 8) = 2/12 = 0.167\n",
    "- P(bad|Positive) = (0 + 1) / (4 + 8) = 1/12 = 0.083\n",
    "- P(quality|Positive) = (0 + 1) / (4 + 8) = 1/12 = 0.083\n",
    "- P(terrible|Positive) = (0 + 1) / (4 + 8) = 1/12 = 0.083\n",
    "- P(experience|Positive) = (0 + 1) / (4 + 8) = 1/12 = 0.083\n",
    "\n",
    "**Similarly for Negative class:**\n",
    "- P(bad|Negative) = (1 + 1) / (4 + 8) = 2/12 = 0.167\n",
    "- P(quality|Negative) = (1 + 1) / (4 + 8) = 2/12 = 0.167\n",
    "- P(terrible|Negative) = (1 + 1) / (4 + 8) = 2/12 = 0.167\n",
    "- P(experience|Negative) = (1 + 1) / (4 + 8) = 2/12 = 0.167\n",
    "- P(great|Negative) = (0 + 1) / (4 + 8) = 1/12 = 0.083\n",
    "- P(product|Negative) = (0 + 1) / (4 + 8) = 1/12 = 0.083\n",
    "- P(excellent|Negative) = (0 + 1) / (4 + 8) = 1/12 = 0.083\n",
    "- P(service|Negative) = (0 + 1) / (4 + 8) = 1/12 = 0.083\n",
    "\n",
    "##### Step 5: Classification\n",
    "\n",
    "Now, let's classify a new text: \"good product\"\n",
    "\n",
    "We calculate:\n",
    "1. P(Positive|\"good product\")\n",
    "2. P(Negative|\"good product\")\n",
    "\n",
    "And choose the class with the higher probability.\n",
    "\n",
    "```math\n",
    "\n",
    "P(Positive|\"good product\") ∝ P(Positive) * P(good|Positive) * P(product|Positive)\n",
    "                            = 0.5 * 0.083 * 0.167\n",
    "                            = 0.00693\n",
    "```\n",
    "\n",
    "```math\n",
    "P(Negative|\"good product\") ∝ P(Negative) * P(good|Negative) * P(product|Negative)\n",
    "                            = 0.5 * 0.083 * 0.083\n",
    "                            = 0.00344\n",
    "```\n",
    "\n",
    "Since 0.00693 > 0.00344, we classify \"good product\" as Positive.\n",
    "\n",
    "#### Why Naive Bayes Works Well for Text Classification\n",
    "\n",
    "1. **Efficiency**: It's fast and requires little training data.\n",
    "2. **Handles high dimensionality**: Works well with the large number of features in text data.\n",
    "3. **Performs well in practice**: Despite its \"naive\" assumption, it often gives good results for text classification.\n",
    "\n",
    "#### Limitations\n",
    "\n",
    "The main limitation is the assumption of feature independence, which is rarely true in real-world text data. However, the algorithm often performs well despite this assumption.\n",
    "\n",
    "\n",
    "#### Training a Text Classifier\n",
    "\n",
    "Now lets see how to train a text classifier with Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omnipostai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
